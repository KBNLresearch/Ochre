{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import codecs\n",
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# get test set\n",
    "with codecs.open('/home/jvdzwaan/data/ocr/datadivision.json', encoding='utf-8') as f:\n",
    "    division = json.load(f)\n",
    "print len(division.get('train'))\n",
    "print division.get('train')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_texts(data_files, data_dir):\n",
    "    raw_text = []\n",
    "    gs = []\n",
    "    ocr = []\n",
    "\n",
    "    for df in data_files:\n",
    "        with codecs.open(os.path.join(data_dir, df), encoding='utf-8') as f:\n",
    "            aligned = json.load(f)\n",
    "\n",
    "        ocr.append(aligned['ocr'])\n",
    "        ocr.append([' '])             # add space between two texts\n",
    "        gs.append(aligned['gs'])\n",
    "        gs.append([' '])              # add space between two texts\n",
    "\n",
    "        raw_text.append(''.join(aligned['ocr']))\n",
    "        raw_text.append(''.join(aligned['gs']))\n",
    "\n",
    "    # Make a single array, containing the character-aligned text of all data\n",
    "    # files\n",
    "    gs_text = [y for x in gs for y in x]\n",
    "    ocr_text = [y for x in ocr for y in x]\n",
    "\n",
    "    return ' '.join(raw_text), gs_text, ocr_text\n",
    "\n",
    "data_dir = '/home/jvdzwaan/data/ocr/'\n",
    "raw_val, gs_val, ocr_val = read_texts(division.get('val'), data_dir)\n",
    "raw_test, gs_test, ocr_test = read_texts(division.get('test'), data_dir)\n",
    "raw_train, gs_train, ocr_train = read_texts(division.get('train'), data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_char_to_int(chars):\n",
    "    return dict((c, i) for i, c in enumerate(chars))\n",
    "\n",
    "raw_text = ''.join([raw_val, raw_test, raw_train])\n",
    "raw_text = raw_text.lower()\n",
    "chars = sorted(list(set(raw_text)))\n",
    "chars.append(u'\\n')                      # padding character\n",
    "char_to_int = get_char_to_int(chars)\n",
    "\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "\n",
    "print n_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_string(char_list, lowercase):\n",
    "    if lowercase:\n",
    "        return u''.join(char_list).lower()\n",
    "    return u''.join(char_list)\n",
    "\n",
    "\n",
    "def create_synced_data(ocr_text, gs_text, char_to_int, n_vocab, seq_length=25,\n",
    "                       batch_size=100, padding_char=u'\\n', lowercase=False, step=1):\n",
    "    \"\"\"Create padded one-hot encoded data sets from text.\n",
    "\n",
    "    A sample consists of seq_length characters from ocr_text\n",
    "    (includes empty characters) (input), and seq_length characters from\n",
    "    gs_text (includes empty characters) (output).\n",
    "    ocr_text and gs_tetxt contain aligned arrays of characters.\n",
    "    Because of the empty characters ('' in the character arrays), the input\n",
    "    and output sequences may not have equal length. Therefore input and\n",
    "    output are padded with a padding character (newline).\n",
    "\n",
    "    Returns:\n",
    "      int: the number of samples in the dataset\n",
    "      generator: generator for one-hot encoded data (so the data doesn't have\n",
    "        to fit in memory)\n",
    "    \"\"\"\n",
    "    dataX = []\n",
    "    dataY = []\n",
    "    text_length = len(ocr_text)\n",
    "    for i in range(0, text_length-seq_length + 1, step):\n",
    "        seq_in = ocr_text[i:i+seq_length]\n",
    "        seq_out = gs_text[i:i+seq_length]\n",
    "        dataX.append(to_string(seq_in, lowercase))\n",
    "        dataY.append(to_string(seq_out, lowercase))\n",
    "    return len(dataX), synced_data_gen(dataX, dataY, seq_length, n_vocab,\n",
    "                                       char_to_int, batch_size, padding_char)\n",
    "\n",
    "\n",
    "def synced_data_gen(dataX, dataY, seq_length, n_vocab, char_to_int, batch_size,\n",
    "                    padding_char):\n",
    "    while 1:\n",
    "        for batch_idx in range(0, len(dataX), batch_size):\n",
    "            X = np.zeros((batch_size, seq_length), dtype=np.int)\n",
    "            Y = np.zeros((batch_size, seq_length, n_vocab), dtype=np.bool)\n",
    "            sliceX = dataX[batch_idx:batch_idx+batch_size]\n",
    "            sliceY = dataY[batch_idx:batch_idx+batch_size]\n",
    "            for i, (sentenceX, sentenceY) in enumerate(zip(sliceX, sliceY)):\n",
    "                for j, c in enumerate(sentenceX):\n",
    "                    X[i, j] = char_to_int[c]\n",
    "                for j in range(seq_length-len(sentenceX)):\n",
    "                    X[i, len(sentenceX) + j] = char_to_int[padding_char]\n",
    "                for j, c in enumerate(sentenceY):\n",
    "                    Y[i, j, char_to_int[c]] = 1\n",
    "                for j in range(seq_length-len(sentenceY)):\n",
    "                    Y[i, len(sentenceY) + j, char_to_int[padding_char]] = 1\n",
    "            yield X, Y\n",
    "\n",
    "lowercase = True\n",
    "batch_size = 100\n",
    "seq_length = 25\n",
    "            \n",
    "numTrainSamples, trainDataGen = create_synced_data(ocr_train, gs_train, char_to_int, n_vocab, seq_length=seq_length, batch_size=batch_size, lowercase=lowercase, step=1)\n",
    "numTestSamples, testDataGen = create_synced_data(ocr_test, gs_test, char_to_int, n_vocab, seq_length=seq_length, batch_size=batch_size, lowercase=lowercase, step=1)\n",
    "numValSamples, valDataGen = create_synced_data(ocr_val, gs_val, char_to_int, n_vocab, seq_length=seq_length, batch_size=batch_size, lowercase=lowercase, step=1)\n",
    "n_patterns = numTrainSamples\n",
    "print(\"Train Patterns: {}\".format(n_patterns))\n",
    "print(\"Validation Patterns: {}\".format(numValSamples))\n",
    "print(\"Test Patterns: {}\".format(numTestSamples))\n",
    "print('Total: {}'.format(numTrainSamples+numTestSamples+numValSamples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numTrainSamples, trainDataGen = create_synced_data(ocr_train, gs_train, char_to_int, n_vocab, seq_length=seq_length, batch_size=batch_size, lowercase=lowercase, step=3)\n",
    "numTestSamples, testDataGen = create_synced_data(ocr_test, gs_test, char_to_int, n_vocab, seq_length=seq_length, batch_size=batch_size, lowercase=lowercase, step=3)\n",
    "numValSamples, valDataGen = create_synced_data(ocr_val, gs_val, char_to_int, n_vocab, seq_length=seq_length, batch_size=batch_size, lowercase=lowercase, step=3)\n",
    "n_patterns = numTrainSamples\n",
    "print(\"Train Patterns: {}\".format(n_patterns))\n",
    "print(\"Validation Patterns: {}\".format(numValSamples))\n",
    "print(\"Test Patterns: {}\".format(numTestSamples))\n",
    "print('Total: {}'.format(numTrainSamples+numTestSamples+numValSamples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Met embedding layer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import Embedding\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(n_vocab, n_vocab, input_length=25))\n",
    "\n",
    "model.add(Bidirectional(LSTM(256, return_sequences=True)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(TimeDistributed(Dense(n_vocab, activation='softmax')))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit_generator(trainDataGen, steps_per_epoch=int(numTrainSamples/batch_size), epochs=40, validation_data=valDataGen, validation_steps=int(numValSamples/batch_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional layer kan alleen bij seq2seq, omdat anders de output van de convulutie niet meer gelijk is aan het aantal output characters, dus dan kun je nooit een sequentie van die lengte voorspellen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv1D, MaxPooling1D\n",
    "\n",
    "filter_length = [5, 3, 3]\n",
    "nb_filter = [196, 196, 256]\n",
    "pool_size = 2\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(n_vocab, n_vocab, input_length=25))\n",
    "\n",
    "for i in range(len(nb_filter)):\n",
    "    model.add(Conv1D(filters=nb_filter[i],\n",
    "                     kernel_size=filter_length[i],\n",
    "                     padding='valid',\n",
    "                     activation='relu',\n",
    "                     kernel_initializer='glorot_normal'))\n",
    "\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "\n",
    "model.add(Bidirectional(LSTM(256)))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(RepeatVector(seq_length))\n",
    "model.add(LSTM(256, return_sequences=True))\n",
    "\n",
    "model.add(TimeDistributed(Dense(n_vocab, activation='softmax')))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit_generator(trainDataGen, steps_per_epoch=int(numTrainSamples/batch_size), epochs=40, validation_data=valDataGen, validation_steps=int(numValSamples/batch_size))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
