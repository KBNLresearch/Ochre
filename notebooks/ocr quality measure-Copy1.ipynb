{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://offbit.github.io/how-to-read/\n",
    "# idea: chunk characters into words using convolutional layers \n",
    "#(ocr mistakes are recognized, because they don't fit into the standard word models)\n",
    "# than do lstm on the chunks\n",
    "# input = representation of document (characters and sentences)\n",
    "# output = OCR quality (high/low or high/medium/low)\n",
    "# gebruiken we WER of CER als quality measure?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import codecs\n",
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# get test set\n",
    "with codecs.open('/home/jvdzwaan/data/ocr/datadivision.json', encoding='utf-8') as f:\n",
    "    division = json.load(f)\n",
    "print len(division.get('train'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlppln.commands.pattern_nl import parse\n",
    "from pattern.nl import parsetree\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def parse_text2(text):\n",
    "    sentences = []\n",
    "    s_idx = 0\n",
    "    s = []\n",
    "\n",
    "    for t in parse(text):\n",
    "        if s_idx != t['sentence']:\n",
    "            sentences.append(s)\n",
    "            s = []\n",
    "            s_idx = t['sentence']\n",
    "        s.append(t['word'])\n",
    "    sentences.append(s)\n",
    "    return sentences\n",
    "\n",
    "def parse_text3(text):\n",
    "    p = parsetree(text,\n",
    "                  tokenize=True,     # Split punctuation marks from words?\n",
    "                  tags=True,         # Parse part-of-speech tags? (NN, JJ, ...)\n",
    "                  chunks=False,      # Parse chunks? (NP, VP, PNP, ...)\n",
    "                  relations=False,   # Parse chunk relations? (-SBJ, -OBJ, ...)\n",
    "                  lemmata=True,      # Parse lemmata? (ate => eat)\n",
    "                  encoding='utf-8',  # Input string encoding.\n",
    "                  tagset=None)       # Penn Treebank II (default) or UNIVERSAL.\n",
    "    for sentence_id, sentence in enumerate(p):\n",
    "        print sentence.string\n",
    "        \n",
    "def parse_text(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    return [s.lower() for s in sentences]\n",
    "\n",
    "print parse_text(\"Dit is een test. Er zijn twee zinnen.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import os\n",
    "\n",
    "def doc_name(doc_id):\n",
    "    return '{}-ds.ocr.txt'.format(doc_id)\n",
    "\n",
    "data_dir = '/home/jvdzwaan/data/dncvu/ocr/'\n",
    "\n",
    "num_sentences = []\n",
    "sentence_lengths = []\n",
    "doc_ids = []\n",
    "docs = []\n",
    "\n",
    "for j in division.get('train'):\n",
    "    doc_id = j.split('.')[0]\n",
    "    doc_ids.append(doc_id)\n",
    "    with codecs.open(os.path.join(data_dir, doc_name(doc_id)), encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    sentences = parse_text(text)\n",
    "    num_sentences.append(len(sentences))\n",
    "    for s in sentences:\n",
    "        sentence_lengths.append(len(s))\n",
    "    docs.append(sentences)\n",
    "\n",
    "print max(num_sentences)\n",
    "print max(sentence_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(num_sentences, bins=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(sentence_lengths, bins=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on the graphs, we pick\n",
    "maxlen = 256\n",
    "max_sentences = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = u''\n",
    "for doc in docs:\n",
    "    for s in doc:\n",
    "        txt += s\n",
    "\n",
    "chars = list(set(txt))\n",
    "chars.append(u'*') # add padding character\n",
    "chars.reverse()\n",
    "print ''.join(chars)\n",
    "print('total chars:', len(chars))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "q = pd.read_csv('/home/jvdzwaan/data/ocr/merged.csv', index_col=0)\n",
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the labels\n",
    "m = 'WER'\n",
    "threshold = 5.0\n",
    "\n",
    "labels = []\n",
    "\n",
    "for doc_id in doc_ids:\n",
    "    if q.loc['{}-ds.gs_out'.format(doc_id)][m] > threshold:\n",
    "        labels.append(0)\n",
    "    else:\n",
    "        labels.append(1)\n",
    "print len(labels)\n",
    "print sum(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need a more balanced data set! (add some gold standard texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.ones((len(docs), max_sentences, maxlen), dtype=np.int64) * -1\n",
    "y = np.array(labels)\n",
    "\n",
    "for i, doc in enumerate(docs):\n",
    "    for j, sentence in enumerate(doc):\n",
    "        if j < max_sentences:\n",
    "            for t, char in enumerate(sentence[-maxlen:]):\n",
    "                X[i, j, (maxlen-1-t)] = char_indices[char]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print X[0, 0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Lambda\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dropout\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers import merge\n",
    "\n",
    "filter_length = [5, 3, 3]\n",
    "nb_filter = [196, 196, 256]\n",
    "pool_length = 2\n",
    "\n",
    "in_sentence = Input(shape=(maxlen,), dtype='int64')\n",
    "# binarize function creates a onehot encoding of each character index\n",
    "embedded = Embedding(maxlen, len(chars))(in_sentence)\n",
    "\n",
    "for i in range(len(nb_filter)):\n",
    "    embedded = Conv1D(nb_filter=nb_filter[i],\n",
    "                            filter_length=filter_length[i],\n",
    "                            border_mode='valid',\n",
    "                            activation='relu',\n",
    "                            kernel_initializer='glorot_normal',\n",
    "                            subsample_length=1)(embedded)\n",
    "\n",
    "    embedded = Dropout(0.1)(embedded)\n",
    "    embedded = MaxPooling1D(pool_length=pool_length)(embedded)\n",
    "\n",
    "forward_sent = LSTM(128, return_sequences=False, dropout_W=0.2, dropout_U=0.2, consume_less='gpu')(embedded)\n",
    "backward_sent = LSTM(128, return_sequences=False, dropout_W=0.2, dropout_U=0.2, consume_less='gpu', go_backwards=True)(embedded)\n",
    "\n",
    "sent_encode = merge([forward_sent, backward_sent], mode='concat', concat_axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.ones((len(docs), max_sentences, maxlen), dtype=np.int64) * -1\n",
    "y = np.array(sentiments)\n",
    "\n",
    "for i, doc in enumerate(docs):\n",
    "    for j, sentence in enumerate(doc):\n",
    "        if j < max_sentences:\n",
    "            for t, char in enumerate(sentence[-maxlen:]):\n",
    "                X[i, j, (maxlen-1-t)] = char_indices[char]"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
