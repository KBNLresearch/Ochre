{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import codecs\n",
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from nlppln.utils import remove_ext\n",
    "\n",
    "data_files = glob.glob('/home/jvdzwaan/data/ocr/*[0-9].json')\n",
    "\n",
    "print 'Number of texts', len(data_files)\n",
    "\n",
    "np.random.seed(4)\n",
    "np.random.shuffle(data_files)\n",
    "\n",
    "n = len(data_files) / 10 / 2\n",
    "print 'Number of texts put in validation and test set', n\n",
    "#n = 1\n",
    "\n",
    "validation_texts = data_files[0:n]\n",
    "test_texts = data_files[n:n+n]\n",
    "train_texts = data_files[n+n:]\n",
    "\n",
    "# save to json file, so it can be reused on DAS5\n",
    "division = {'train': [remove_ext(os.path.basename(t)) for t in train_texts], \n",
    "            'val': [remove_ext(os.path.basename(t)) for t in validation_texts],\n",
    "            'test': [remove_ext(os.path.basename(t)) for t in test_texts]}\n",
    "with codecs.open('/home/jvdzwaan/data/ocr/datadivision2.json', 'wb', encoding='utf-8') as f:\n",
    "    json.dump(division, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with codecs.open('/home/jvdzwaan/data/ocr/datadivision.json', encoding='utf-8') as f:\n",
    "    division = json.load(f)\n",
    "print division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import codecs\n",
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def read_texts(data_files, data_dir):\n",
    "    raw_text = []\n",
    "    gs = []\n",
    "    ocr = []\n",
    "    \n",
    "    for df in data_files:\n",
    "        with codecs.open(os.path.join(data_dir, df), encoding='utf-8') as f:\n",
    "            aligned = json.load(f)\n",
    "        \n",
    "        ocr.append(aligned['ocr'])\n",
    "        gs.append(aligned['gs'])\n",
    "        \n",
    "        raw_text.append(''.join(aligned['ocr']))\n",
    "        raw_text.append(''.join(aligned['gs']))\n",
    "    return ' '.join(raw_text), gs, ocr\n",
    "    \n",
    "seq_length = 25\n",
    "\n",
    "raw_val, gs_val, ocr_val = read_texts(division['val'], '/home/jvdzwaan/data/ocr')\n",
    "raw_test, gs_test, ocr_test = read_texts(division['test'], '/home/jvdzwaan/data/ocr')\n",
    "raw_train, gs_train, ocr_train = read_texts(division['train'], '/home/jvdzwaan/data/ocr')\n",
    "\n",
    "raw_text = ''.join([raw_val, raw_test, raw_train])\n",
    "\n",
    "chars = sorted(list(set(raw_text)))\n",
    "chars.append(u'\\n')\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))    \n",
    "\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print \"Total Characters: \", n_chars\n",
    "print \"Total Vocab: \", n_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data(ocr_texts, gs_texts, seq_length=25):\n",
    "    \"\"\"Create padded one-hot encoded data sets from text.\n",
    "    \n",
    "    A sample consists of seq_length characters from texts from ocr_texts \n",
    "    (includes empty characters) (input), and seq_length characters from \n",
    "    gs_texts (includes empty characters) (output).\n",
    "    ocr_texts and gs_tetxts contain aligned arrays of characters.\n",
    "    Because of the empty characters ('' in the character arrays), the input\n",
    "    and output sequences may not have equal length. Therefore input and \n",
    "    output are padded with a padding character (newline).\n",
    "    \"\"\"\n",
    "    dataX = []\n",
    "    dataY = []\n",
    "    for ocr, gs in zip(ocr_texts, gs_texts):\n",
    "        text_length = len(ocr)\n",
    "        for i in range(0, text_length-seq_length +1, 1):\n",
    "            seq_in = ocr[i:i+seq_length]\n",
    "            seq_out = gs[i:i+seq_length]\n",
    "            dataX.append(''.join(seq_in))\n",
    "            dataY.append(''.join(seq_out))\n",
    "    X = np.zeros((len(dataX), seq_length, n_vocab), dtype=np.bool)\n",
    "    Y = np.zeros((len(dataY), seq_length, n_vocab), dtype=np.bool)\n",
    "\n",
    "    for i, sentence in enumerate(dataX):\n",
    "        for j, c in enumerate(sentence):\n",
    "            X[i, j, char_to_int[c]] = 1\n",
    "        for j in range(seq_length-len(sentence)):\n",
    "            X[i, len(sentence) + j, char_to_int[u'\\n']] = 1\n",
    "            #print len(sentence)+j\n",
    "        #print X[i]\n",
    "        #print X[i].shape\n",
    "\n",
    "    for i, sentence in enumerate(dataY):\n",
    "        #print sentence\n",
    "        for j, c in enumerate(sentence):\n",
    "            Y[i, j, char_to_int[c]] = 1\n",
    "        for j in range(seq_length-len(sentence)):\n",
    "            Y[i, len(sentence)+j, char_to_int[u'\\n']] = 1\n",
    "        #print Y[i]\n",
    "        #print Y[i].shape\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_data(data):\n",
    "    res = xTrain.sum(axis=2)\n",
    "    b = np.ones(res.shape, dtype=np.int)\n",
    "    \n",
    "    return (res==b).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrain, yTrain = create_data(ocr_train, gs_train)\n",
    "xTest, yTest = create_data(ocr_test, gs_test)\n",
    "xVal, yVal = create_data(ocr_val, gs_val)\n",
    "\n",
    "print 'Train data OK?:', check_data(xTrain), check_data(yTrain)\n",
    "print 'Test data OK?:', check_data(xTest), check_data(yTest)\n",
    "print 'Val data OK?:', check_data(xVal), check_data(yVal)\n",
    "\n",
    "n_patterns = len(xTrain)\n",
    "print \"Total Patterns: \", n_patterns\n",
    "print 'val + test + train', len(xTrain) + len(xVal) + len(xTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import RepeatVector\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils\n",
    "\n",
    "LAYERS = 1\n",
    "NODES = 256\n",
    "\n",
    "model = Sequential()\n",
    "# \"Encode\" the input sequence using an RNN, producing an output of HIDDEN_SIZE\n",
    "# note: in a situation where your input sequences have a variable length,\n",
    "# use input_shape=(None, nb_feature).\n",
    "model.add(LSTM(NODES, input_shape=(seq_length, len(chars)), return_sequences=True))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(LSTM(NODES, return_sequences=True))\n",
    "model.add(Dropout(0.5))\n",
    "# For the decoder's input, we repeat the encoded input for each time step\n",
    "#model.add(RepeatVector(25))\n",
    "# The decoder RNN could be multiple layers stacked or a single layer\n",
    "#for _ in range(LAYERS):\n",
    "    #model.add(LSTM(NODES, return_sequences=True))\n",
    "\n",
    "# For each of step of the output sequence, decide which character should be chosen\n",
    "model.add(TimeDistributed(Dense(len(chars), activation='softmax')))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the checkpoint\n",
    "filepath=\"/home/jvdzwaan/data/tmp/dncvu-ad-aligned/padded-256-seed4-{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100\n",
    "model.fit(xTrain, yTrain, batch_size=BATCH_SIZE, epochs=50, validation_data=(xVal, yVal), callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the network weights\n",
    "filename = \"/home/jvdzwaan/data/tmp/dncvu-ad-aligned/padded-256-02-0.3721.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int_to_char = dict((i, c) for i, c in enumerate(chars))\n",
    "print int_to_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for vector in yVal[0:1,:,:]:\n",
    "    indices = np.where(vector==True)[1]\n",
    "    for i in indices:\n",
    "        print int_to_char[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model.predict(xVal)\n",
    "print predicted.shape\n",
    "#for i in range(len(xVal)):   \n",
    "#    r=model.predict(xVal[0:1,:,:])\n",
    "#    for vector in r:\n",
    "#        for p in vector:\n",
    "#            i = np.random.choice(n_vocab, p=p)\n",
    "#            print int_to_char[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match = 0\n",
    "no_match = 0\n",
    "in_is_out = 0\n",
    "for i, sequence in enumerate(predicted):\n",
    "    predicted_indices = [np.random.choice(n_vocab, p=p) for p in sequence]\n",
    "    indices = np.where(yVal[i:i+1,:,:]==True)[2]\n",
    "    if predicted_indices != list(indices):\n",
    "        no_match += 1\n",
    "        pred_str = u''.join([int_to_char[j] for j in predicted_indices])\n",
    "        pred_str = pred_str.replace(u'\\n', u'')\n",
    "        \n",
    "        gs = u''.join([int_to_char[j] for j in indices])\n",
    "        gs = gs.replace(u'\\n', u'')\n",
    "        #print pred_str\n",
    "        #print gs\n",
    "        print u'\"{}\"\\t\"{}\"'.format(gs, pred_str)\n",
    "    else:\n",
    "        match += 1\n",
    "    indices2 = np.where(xVal[i:i+1,:,:]==True)[2]\n",
    "    if list(indices) == list(indices2):\n",
    "        in_is_out += 1\n",
    "        \n",
    "print 'Match', match\n",
    "print 'No match', no_match\n",
    "print 'Input == output', in_is_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[0,0,0,0,0],[0,0,0,0,0]])\n",
    "np.where(a==1)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
