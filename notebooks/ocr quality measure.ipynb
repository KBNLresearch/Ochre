{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://offbit.github.io/how-to-read/\n",
    "# idea: chunk characters into words using convolutional layers \n",
    "#(ocr mistakes are recognized, because they don't fit into the standard word models)\n",
    "# than do lstm on the chunks\n",
    "# input = representation of document (characters and sentences)\n",
    "# output = OCR quality (high/low or high/medium/low)\n",
    "# gebruiken we WER of CER als quality measure?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import codecs\n",
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# get test set\n",
    "with codecs.open('/home/jvdzwaan/data/ocr/datadivision.json', encoding='utf-8') as f:\n",
    "    division = json.load(f)\n",
    "print len(division.get('train'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlppln.commands.pattern_nl import parse\n",
    "from pattern.nl import parsetree\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def parse_text2(text):\n",
    "    sentences = []\n",
    "    s_idx = 0\n",
    "    s = []\n",
    "\n",
    "    for t in parse(text):\n",
    "        if s_idx != t['sentence']:\n",
    "            sentences.append(s)\n",
    "            s = []\n",
    "            s_idx = t['sentence']\n",
    "        s.append(t['word'])\n",
    "    sentences.append(s)\n",
    "    return sentences\n",
    "\n",
    "def parse_text3(text):\n",
    "    p = parsetree(text,\n",
    "                  tokenize=True,     # Split punctuation marks from words?\n",
    "                  tags=True,         # Parse part-of-speech tags? (NN, JJ, ...)\n",
    "                  chunks=False,      # Parse chunks? (NP, VP, PNP, ...)\n",
    "                  relations=False,   # Parse chunk relations? (-SBJ, -OBJ, ...)\n",
    "                  lemmata=True,      # Parse lemmata? (ate => eat)\n",
    "                  encoding='utf-8',  # Input string encoding.\n",
    "                  tagset=None)       # Penn Treebank II (default) or UNIVERSAL.\n",
    "    for sentence_id, sentence in enumerate(p):\n",
    "        print sentence.string\n",
    "        \n",
    "def parse_text(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    return [s.lower() for s in sentences]\n",
    "\n",
    "print parse_text(\"Dit is een test. Er zijn twee zinnen.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import os\n",
    "\n",
    "def doc_name(doc_id):\n",
    "    return '{}-ds.ocr.txt'.format(doc_id)\n",
    "\n",
    "data_dir = '/home/jvdzwaan/data/dncvu/ocr/'\n",
    "\n",
    "num_sentences = []\n",
    "sentence_lengths = []\n",
    "doc_ids = []\n",
    "docs = []\n",
    "\n",
    "for j in division.get('train'):\n",
    "    doc_id = j.split('.')[0]\n",
    "    doc_ids.append(doc_id)\n",
    "    with codecs.open(os.path.join(data_dir, doc_name(doc_id)), encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "    sentences = parse_text(text)\n",
    "    num_sentences.append(len(sentences))\n",
    "    for s in sentences:\n",
    "        sentence_lengths.append(len(s))\n",
    "    docs.append(sentences)\n",
    "\n",
    "print max(num_sentences)\n",
    "print max(sentence_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(num_sentences, bins=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(sentence_lengths, bins=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on the graphs, we pick\n",
    "maxlen = 256\n",
    "max_sentences = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = u''\n",
    "for doc in docs:\n",
    "    for s in doc:\n",
    "        txt += s\n",
    "\n",
    "chars = list(set(txt))\n",
    "chars.append(u'*') # add padding character\n",
    "chars.reverse()\n",
    "print ''.join(chars)\n",
    "print('total chars:', len(chars))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "q = pd.read_csv('/home/jvdzwaan/data/ocr/merged.csv', index_col=0)\n",
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the labels\n",
    "m = 'CER'\n",
    "threshold = 1.0\n",
    "\n",
    "labels = []\n",
    "\n",
    "for doc_id in doc_ids:\n",
    "    if q.loc['{}-ds.gs_out'.format(doc_id)][m] > threshold:\n",
    "        labels.append(0)\n",
    "    else:\n",
    "        labels.append(1)\n",
    "print len(labels)\n",
    "print sum(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a more balanced data set\n",
    "\n",
    "def balanced_set(performance_df, doc_ids, m='CER', threshold=1.0, seed=4):\n",
    "\n",
    "    high = []\n",
    "    low = []\n",
    "\n",
    "    for doc_id in doc_ids:\n",
    "        if performance_df.loc['{}-ds.gs_out'.format(doc_id)][m] > threshold:\n",
    "            # low quality\n",
    "            low.append(doc_id)\n",
    "            # high quality\n",
    "        else:\n",
    "            high.append(doc_id)\n",
    "    print 'high quality', len(high)\n",
    "    print 'low quality', len(low)\n",
    "    \n",
    "    # calculate the number of texts for which the gs data should be added to the data\n",
    "    num_gs = len(doc_ids)/2 - len(high)\n",
    "    print 'num gs', num_gs\n",
    "    \n",
    "    # determine what gs texts should be added to the data\n",
    "    low.sort()\n",
    "\n",
    "    np.random.seed(seed)\n",
    "    np.random.shuffle(low)\n",
    "\n",
    "    gs = low[0:num_gs]\n",
    "    low = low[num_gs:]\n",
    "    \n",
    "    print 'high quality', len(high)\n",
    "    print 'low quality', len(low)\n",
    "    print 'gs', len(gs)\n",
    "    \n",
    "    return high, low, gs\n",
    "\n",
    "high, low, gs = balanced_set(q, doc_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_texts(file_prefixes, high, low, gs, data_dir):\n",
    "    raw = []\n",
    "    docs = []\n",
    "    labels = []\n",
    "\n",
    "    for p in file_prefixes:\n",
    "        with codecs.open(os.path.join(data_dir, '{}.json'.format(p)), encoding='utf-8') as f:\n",
    "            aligned = json.load(f)\n",
    "\n",
    "        if p in high:\n",
    "            text = ''.join(aligned['ocr'])\n",
    "            label = 1\n",
    "        elif p in low:\n",
    "            text = ''.join(aligned['ocr'])\n",
    "            label = 0\n",
    "        else:\n",
    "            text = ''.join(aligned['gs'])\n",
    "            label = 1\n",
    "\n",
    "        raw.append(text)\n",
    "        docs.append(parse_text(text))\n",
    "        labels.append(label)\n",
    "\n",
    "    return ''.join(raw), docs, labels\n",
    "\n",
    "data_dir = '/home/jvdzwaan/data/dncvu/aligned/'\n",
    "high, low, gs = balanced_set(q, doc_ids)\n",
    "raw_train, docs_train, labels_train = read_texts(doc_ids, high, low, gs, data_dir)\n",
    "for j in division.get('val'):\n",
    "    doc_id = j.split('.')[0]\n",
    "    doc_ids.append(doc_id)\n",
    "high, low, gs = balanced_set(q, doc_ids)\n",
    "raw_val, docs_val, labels_val = read_texts(doc_ids, high, low, gs, data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print doc_ids[0]\n",
    "print docs_train[0]\n",
    "print labels[0]\n",
    "# parse_text maakt lower_case. Dat willen we misschien niet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.zeros((len(docs), max_sentences, maxlen), dtype=np.int64)\n",
    "y = np.array(labels)\n",
    "\n",
    "for i, doc in enumerate(docs):\n",
    "    for j, sentence in enumerate(doc):\n",
    "        if j < max_sentences:\n",
    "            for t, char in enumerate(sentence[-maxlen:]):\n",
    "                X[i, j, (maxlen-1-t)] = char_indices[char]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print X[0, 0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Lambda\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers.convolutional import Conv1D, MaxPooling1D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.core import Dropout, Dense\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers import merge\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "\n",
    "filter_length = [5, 3, 3]\n",
    "nb_filter = [196, 196, 256]\n",
    "pool_length = 2\n",
    "\n",
    "in_sentence = Input(shape=(maxlen,), dtype='int64')\n",
    "# binarize function creates a onehot encoding of each character index\n",
    "embedded = Embedding(maxlen, len(chars))(in_sentence)\n",
    "\n",
    "for i in range(len(nb_filter)):\n",
    "    embedded = Conv1D(nb_filter=nb_filter[i],\n",
    "                            filter_length=filter_length[i],\n",
    "                            border_mode='valid',\n",
    "                            activation='relu',\n",
    "                            kernel_initializer='glorot_normal',\n",
    "                            subsample_length=1)(embedded)\n",
    "\n",
    "    embedded = Dropout(0.1)(embedded)\n",
    "    embedded = MaxPooling1D(pool_length=pool_length)(embedded)\n",
    "\n",
    "forward_sent = LSTM(128, return_sequences=False, dropout_W=0.2, dropout_U=0.2, consume_less='gpu')(embedded)\n",
    "backward_sent = LSTM(128, return_sequences=False, dropout_W=0.2, dropout_U=0.2, consume_less='gpu', go_backwards=True)(embedded)\n",
    "\n",
    "sent_encode = merge([forward_sent, backward_sent], mode='concat', concat_axis=-1)\n",
    "sent_encode = Dropout(0.3)(sent_encode)\n",
    "\n",
    "encoder = Model(input=in_sentence, output=sent_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = Input(shape=(max_sentences, maxlen), dtype='int64')\n",
    "encoded = TimeDistributed(encoder)(sequence)\n",
    "forwards = LSTM(80, return_sequences=False, dropout_W=0.2, dropout_U=0.2, consume_less='gpu')(encoded)\n",
    "backwards = LSTM(80, return_sequences=False, dropout_W=0.2, dropout_U=0.2, consume_less='gpu', go_backwards=True)(encoded)\n",
    "\n",
    "merged = merge([forwards, backwards], mode='concat', concat_axis=-1)\n",
    "output = Dropout(0.3)(merged)\n",
    "output = Dense(128, activation='relu')(output)\n",
    "output = Dropout(0.3)(output)\n",
    "output = Dense(1, activation='sigmoid')(output)\n",
    "\n",
    "model = Model(input=sequence, output=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, labels, batch_size=64, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo:\n",
    "# - make data sets (more) balanced\n",
    "# - add validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://machinelearningmastery.com/timedistributed-layer-for-long-short-term-memory-networks-in-python/\n",
    "# http://minimaxir.com/2017/04/char-embeddings/"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
