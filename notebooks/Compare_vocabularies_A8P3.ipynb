{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ground truth vocabulary + number of hapaxes\n",
    "\n",
    "# lees in gt files\n",
    "# tokenize\n",
    "# create terms document matrix (using sklearn)\n",
    "# get vocabulary\n",
    "# calculate corpus frequencies\n",
    "# count hapaxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import nltk.data\n",
    "\n",
    "from itertools import chain\n",
    "\n",
    "from nltk import word_tokenize\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def nltk_tokenize(texts_file, punkt='tokenizers/punkt/dutch.pickle'):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        texts_file (str): File name of a file that contains the texts. This\n",
    "            should contain one document per line.\n",
    "        punkt (str): Path to the nltk punctuation data to be used.\n",
    "\n",
    "    Yields:\n",
    "        Counter: term-frequency vector representing a document.\n",
    "    \"\"\"\n",
    "    tokenizer = nltk.data.load(punkt)\n",
    "    \n",
    "    with open(texts_file) as f:\n",
    "        text = f.read()\n",
    "        tokens = [word_tokenize(sent)\n",
    "                  for sent in tokenizer.tokenize(text)]\n",
    "\n",
    "        return list(chain(*tokens))\n",
    "   \n",
    "\n",
    "def do_nothing(list_of_words):\n",
    "    return list_of_words\n",
    "\n",
    "def terms_documents_matrix_word_lists(word_lists):\n",
    "    \"\"\"Returns a terms document matrix and related objects of a corpus\n",
    "\n",
    "    Inputs:\n",
    "        word_lists: iterator over lists of words\n",
    "    Returns:\n",
    "        corpus: a sparse terms documents matrix\n",
    "        v: the vecorizer object containing the vocabulary (i.e., all word forms\n",
    "            in the corpus)\n",
    "    \"\"\"\n",
    "    v = CountVectorizer(tokenizer=do_nothing, lowercase=False)\n",
    "    corpus = v.fit_transform(word_lists)\n",
    "\n",
    "    return corpus, v\n",
    "\n",
    "\n",
    "def get_datadivision(json_file):\n",
    "    with open(json_file) as f:\n",
    "        return json.load(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from nlppln.utils import get_files\n",
    "from ochre.utils import get_files as get_test_files\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "def texts_iterator(in_files):    \n",
    "    for in_file in in_files:\n",
    "        yield(nltk_tokenize(in_file))\n",
    "\n",
    "json_file = '/home/jvdzwaan/data/kb-ocr/text_aligned_blocks-match_gs/datadivision-A8P3.json'\n",
    "        \n",
    "in_dir = '/home/jvdzwaan/data/kb-ocr/text_aligned_blocks-match_gs/gs/'\n",
    "#in_files = get_files(in_dir)\n",
    "div = get_datadivision(json_file)\n",
    "in_files = get_test_files(in_dir, div, 'test')\n",
    "print(len(in_files))\n",
    "gs_corpus, gs_vocabulary = terms_documents_matrix_word_lists(texts_iterator(in_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "in_dir = '/home/jvdzwaan/data/kb-ocr/text_aligned_blocks-match_gs/ocr/'\n",
    "#in_files = get_files(in_dir)\n",
    "in_files = get_test_files(in_dir, div, 'test')\n",
    "print(len(in_files))\n",
    "ocr_corpus, ocr_vocabulary = terms_documents_matrix_word_lists(texts_iterator(in_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "in_dir = '/home/jvdzwaan/data/kb-ocr/text_aligned_blocks-match_gs/pred-A8P3/'\n",
    "#in_files = get_files(in_dir)\n",
    "in_files = get_test_files(in_dir, div, 'test')\n",
    "print(len(in_files))\n",
    "#print(in_files)\n",
    "pred_corpus, pred_vocabulary = terms_documents_matrix_word_lists(texts_iterator(in_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('# types in gs', len(gs_vocabulary.get_feature_names()))\n",
    "print('# types in ocr', len(ocr_vocabulary.get_feature_names()))\n",
    "print('# types in pred', len(pred_vocabulary.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_voc = set(gs_vocabulary.get_feature_names())\n",
    "ocr_voc = set(ocr_vocabulary.get_feature_names())\n",
    "pred_voc = set(pred_vocabulary.get_feature_names()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ocr_voc.difference(pred_voc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(gs_voc.intersection(ocr_voc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(gs_voc.intersection(pred_voc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ocr_voc.intersection(pred_voc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib_venn import venn2\n",
    "venn2(subsets = [gs_voc, pred_voc], set_labels=['GT vocabulary', 'Corrected OCR vocabulary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "venn2(subsets = [gs_voc, ocr_voc], set_labels=['GT vocabulary', 'Original OCR vocabulary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "venn2(subsets = [ocr_voc, pred_voc], set_labels=['Original OCR vocabulary', 'Corrected OCR vocabulary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "def get_hapaxes(corpus, vectorizer):\n",
    "    cx = scipy.sparse.csr_matrix(corpus)\n",
    "    print(cx.shape)\n",
    "    word_counts = cx.sum(axis=0)  # sum the colums\n",
    "    wc_list = np.array(word_counts).flatten().tolist()\n",
    "    \n",
    "    if len(wc_list) != len(vectorizer.get_feature_names()):\n",
    "        print('Unequal lengths')\n",
    "        print('wc_list', len(wc_list))\n",
    "        print('vocabulary', len(vectorizer.get_feature_names()))\n",
    "    else:\n",
    "        hapaxes = []\n",
    "        for word, freq in zip(vectorizer.get_feature_names(), wc_list):\n",
    "            if freq == 1:\n",
    "                hapaxes.append(word)\n",
    "        print('# hapaxes:', len(hapaxes))\n",
    "        \n",
    "        return(set(hapaxes))\n",
    "\n",
    "print('GS')\n",
    "gs_hapaxes = get_hapaxes(gs_corpus, gs_vocabulary)\n",
    "print('OCR')\n",
    "ocr_hapaxes = get_hapaxes(ocr_corpus, ocr_vocabulary)\n",
    "print('CORR')\n",
    "pred_hapaxes = get_hapaxes(pred_corpus, pred_vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_hapaxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tv in ocr_corpus:\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
