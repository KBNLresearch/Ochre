{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import glob\n",
    "\n",
    "ocr_texts = []\n",
    "gs_texts = []\n",
    "\n",
    "ocr_lengths = []\n",
    "gs_lengths = []\n",
    "\n",
    "\n",
    "ocr_files = glob.glob('/home/jvdzwaan/data/ocr/BAO*.ocr.txt')\n",
    "ocr_files.sort()\n",
    "gs_files = glob.glob('/home/jvdzwaan/data/ocr/BAO*.gs.txt')\n",
    "gs_files.sort()\n",
    "\n",
    "for ocr_file, gs_file in zip(ocr_files, gs_files):\n",
    "    #print ocr_file\n",
    "    #print gs_file\n",
    "    with codecs.open(ocr_file, encoding='utf-8') as f:\n",
    "        ocr_text = f.read()\n",
    "    ocr_texts.append(ocr_text.lower())\n",
    "    ocr_lengths.append(len(ocr_text))\n",
    "    \n",
    "    with codecs.open(gs_file, encoding='utf-8') as f:\n",
    "        gs_text = f.read()\n",
    "    gs_texts.append(gs_text.lower())\n",
    "    gs_lengths.append(len(gs_text))\n",
    "    #print len(ocr_text), len(gs_text)\n",
    "    \n",
    "print 'Number of newspaper articles:', len(ocr_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_text = []\n",
    "for ocr, gs in zip(ocr_texts, gs_texts):\n",
    "    raw_text.append(ocr)\n",
    "    raw_text.append(gs)\n",
    "raw_text.append('\\n')\n",
    "\n",
    "raw_text = ' '.join(raw_text)\n",
    "\n",
    "chars = sorted(list(set(raw_text)))\n",
    "char_to_int = dict((c, i) for i, c in enumerate(chars))    \n",
    "\n",
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print \"Total Characters: \", n_chars\n",
    "print \"Total Vocab: \", n_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print chars\n",
    "print char_to_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "seq_length = 25\n",
    "dataX = []\n",
    "dataY = []\n",
    "for ocr, gs in zip(ocr_texts, gs_texts):\n",
    "    text_length = max(len(ocr), len(gs))\n",
    "    #print text_length\n",
    "    #print len(ocr),\n",
    "    if len(ocr) < text_length:\n",
    "        ocr = ocr + '\\n' * (text_length-len(ocr))\n",
    "    #print len(ocr)\n",
    "    #print len(gs),\n",
    "    if len(gs) < text_length:\n",
    "        gs = gs + '\\n' * (text_length-len(gs))\n",
    "    #print len(gs)\n",
    "    for i in range(0, text_length-seq_length +1, 1):\n",
    "        seq_in = ocr[i:i+seq_length]\n",
    "        seq_out = gs[i:i+seq_length]\n",
    "        dataX.append(seq_in)\n",
    "        dataY.append(seq_out)\n",
    "\n",
    "X = np.zeros((len(dataX), seq_length, n_vocab), dtype=np.bool)\n",
    "Y = np.zeros((len(dataY), seq_length, n_vocab), dtype=np.bool)\n",
    "\n",
    "for i, sentence in enumerate(dataX):\n",
    "    #print sentence\n",
    "    for j, c in enumerate(sentence):\n",
    "        X[i, j, char_to_int[c]] = 1\n",
    "    #print X[i]\n",
    "    #print X[i].shape\n",
    "\n",
    "for i, sentence in enumerate(dataY):\n",
    "    #print sentence\n",
    "    for j, c in enumerate(sentence):\n",
    "        Y[i, j, char_to_int[c]] = 1\n",
    "    #print Y[i]\n",
    "    #print Y[i].shape\n",
    "\n",
    "n_patterns = len(dataX)\n",
    "print \"Total Patterns: \", n_patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import RepeatVector\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAYERS = 1\n",
    "\n",
    "model = Sequential()\n",
    "# \"Encode\" the input sequence using an RNN, producing an output of HIDDEN_SIZE\n",
    "# note: in a situation where your input sequences have a variable length,\n",
    "# use input_shape=(None, nb_feature).\n",
    "model.add(LSTM(seq_length, input_shape=(seq_length, len(chars))))\n",
    "# For the decoder's input, we repeat the encoded input for each time step\n",
    "model.add(RepeatVector(seq_length))\n",
    "# The decoder RNN could be multiple layers stacked or a single layer\n",
    "for _ in range(LAYERS):\n",
    "    model.add(LSTM(seq_length, return_sequences=True))\n",
    "\n",
    "# For each of step of the output sequence, decide which character should be chosen\n",
    "model.add(TimeDistributed(Dense(len(chars), activation='softmax')))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the checkpoint\n",
    "filepath=\"/home/jvdzwaan/data/tmp/dncvu-ad-mtm/{epoch:02d}-{loss:.4f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, Y, epochs=20, batch_size=100, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the network weights\n",
    "filename = \"/home/jvdzwaan/data/tmp/dncvu-ad-mtm/00-0.0497.hdf5\"\n",
    "model.load_weights(filename)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
