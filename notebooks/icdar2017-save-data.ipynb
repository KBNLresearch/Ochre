{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from ochre.utils import read_texts\n",
    "\n",
    "datasets = '/home/jvdzwaan/data/icdar2017st/eng_monograph/datadivision-sample.json'\n",
    "data_dir = '/home/jvdzwaan/data/icdar2017st/eng_monograph/aligned/'\n",
    "\n",
    "with open(datasets) as d:\n",
    "    division = json.load(d)\n",
    "print(len(division['train']))\n",
    "print(len(division['test']))\n",
    "print(len(division['val']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ochre.utils import get_chars, get_sequences\n",
    "\n",
    "seq_length = 53\n",
    "\n",
    "raw_val, gs_val, ocr_val = read_texts(division.get('val'), data_dir)\n",
    "raw_test, gs_test, ocr_test = read_texts(division.get('test'), data_dir)\n",
    "raw_train, gs_train, ocr_train = read_texts(division.get('train'), data_dir)\n",
    "\n",
    "chars, num_chars, ci = get_chars(raw_val, raw_test, raw_train, False)\n",
    "\n",
    "gs_seqs_val, ocr_seqs_val = get_sequences(gs_val, ocr_val, seq_length)\n",
    "gs_seqs_test, ocr_seqs_test = get_sequences(gs_test, ocr_test, seq_length)\n",
    "gs_seqs_train, ocr_seqs_train = get_sequences(gs_train, ocr_train, seq_length)\n",
    "\n",
    "print('n samples val', len(gs_seqs_val))\n",
    "print('n samples test', len(gs_seqs_test))\n",
    "print('n samples train', len(gs_seqs_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_seqs_val[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ocr_seqs_val[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in gs_seqs_test:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_ocr_space(ocr_text, gs_seqs, ocr_seqs):\n",
    "    ocr_selected = []\n",
    "    gs_selected = []\n",
    "    indices = []\n",
    "    for i, c in enumerate(ocr_text):\n",
    "        if c == ' ':\n",
    "            if i < len(ocr_text)-1 and ocr_text[i+1] != ' ':\n",
    "                try:\n",
    "                    ocr_selected.append(ocr_seqs[i+1])\n",
    "                    gs_selected.append(gs_seqs[i+1])\n",
    "                    #print(repr(ocr_seqs_test[i+1]))\n",
    "                    indices.append(i+1)\n",
    "                except IndexError:\n",
    "                    break\n",
    "    return gs_selected, ocr_selected, indices\n",
    "gs_selected_test, ocr_selected_test, indices = filter_ocr_space(ocr_test, gs_seqs_test, ocr_seqs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ocr_seqs_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for gs, ocr, index in zip(gs_selected_test, ocr_selected_test, indices):\n",
    "    print(gs)\n",
    "    print(ocr)\n",
    "    print(index)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ocr_selected_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(ocr_train), len(ocr_selected))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(gs_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_selected[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ocr_selected[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_selected_test, ocr_selected_test = filter_ocr_space(ocr_test, gs_seqs_test, ocr_seqs_test)\n",
    "gs_selected_train, ocr_selected_train = filter_ocr_space(ocr_train, gs_seqs_train, ocr_seqs_train)\n",
    "gs_selected_val, ocr_selected_val = filter_ocr_space(ocr_val, gs_seqs_val, ocr_seqs_val)\n",
    "\n",
    "print('n samples val', len(gs_selected_val))\n",
    "print('n samples test', len(gs_selected_test))\n",
    "print('n samples train', len(gs_selected_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('train.pkl', 'wb') as f:\n",
    "    pickle.dump((gs_selected_train, ocr_selected_train), f)\n",
    "    \n",
    "with open('val.pkl', 'wb') as f:\n",
    "    pickle.dump((gs_selected_val, ocr_selected_val), f)\n",
    "    \n",
    "with open('ci.pkl', 'wb') as f:\n",
    "    pickle.dump(ci, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_space_tokenized(string):\n",
    "    result = []\n",
    "    for c in string:\n",
    "        if c != ' ':\n",
    "            result.append(c)\n",
    "        else:\n",
    "            result.append('<SPACE>')\n",
    "    return ' '.join(result)\n",
    "to_space_tokenized('Dit is een test.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(gs_seqs, ocr_seqs, name):\n",
    "    with open('{}.ocr'.format(name), 'w') as f:\n",
    "        for s in ocr_seqs:\n",
    "            f.write(to_space_tokenized(s))\n",
    "            f.write('\\n')\n",
    "    with open('{}.gs'.format(name), 'w') as f:\n",
    "        for s in gs_seqs:\n",
    "            f.write(to_space_tokenized(s))\n",
    "            f.write('\\n')\n",
    "\n",
    "save(gs_selected_train, ocr_selected_train, 'train')\n",
    "save(gs_selected_test, ocr_selected_test, 'test')\n",
    "save(gs_selected_val, ocr_selected_val, 'val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide in words: space in ocr and gs means word boundary\n",
    "import json\n",
    "\n",
    "def get_words(aligned_file):\n",
    "    with open(aligned_file, encoding='utf-8') as f:\n",
    "            aligned = json.load(f)\n",
    "            \n",
    "    #print(len(aligned['gs']), len(aligned['ocr']))\n",
    "    \n",
    "    gs_words = []\n",
    "    ocr_words = []\n",
    "\n",
    "    gs_word = []\n",
    "    ocr_word = []\n",
    "\n",
    "    for gs_c, ocr_c in zip(aligned['gs'], aligned['ocr']):\n",
    "        if gs_c == ' ' and ocr_c == ' ':\n",
    "            gs_words.append(''.join(gs_word))\n",
    "            ocr_words.append(''.join(ocr_word))\n",
    "            \n",
    "            gs_word = []\n",
    "            ocr_word = []\n",
    "        else:\n",
    "            gs_word.append(gs_c)\n",
    "            ocr_word.append(ocr_c)\n",
    "            \n",
    "    return gs_words, ocr_words\n",
    "            \n",
    "gs_words, ocr_words = get_words('/home/jvdzwaan/data/icdar2017st/eng_monograph/aligned/1.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Amrhein and Clematide use two preceding and one succeeding word as context sensitive input data (max seq. length is 53)\n",
    "import itertools\n",
    "import os\n",
    "\n",
    "def ngrams(tokens, n=4):\n",
    "    ngrams = zip(*[tokens[i:] for i in range(n)])\n",
    "    return [\" \".join(ngram).strip() for ngram in ngrams]\n",
    "\n",
    "def create_context_data(data_files, name, max_len=53):\n",
    "    \n",
    "    with open('{}.gs'.format(name), 'w') as gs, open('{}.ocr'.format(name), 'w') as ocr:\n",
    "        for in_file in data_files:\n",
    "            gs_words, ocr_words = get_words(in_file)\n",
    "            \n",
    "            for ngram in ngrams(gs_words):\n",
    "                gs.write(to_space_tokenized(ngram[:max_len]))\n",
    "                gs.write('\\n')\n",
    "            \n",
    "            for ngram in ngrams(ocr_words):\n",
    "                ocr.write(to_space_tokenized(ngram[:max_len]))\n",
    "                ocr.write('\\n')\n",
    "                \n",
    "datasets = ('/home/jvdzwaan/data/icdar2017st/eng_monograph/datadivision.json',\n",
    "            '/home/jvdzwaan/data/icdar2017st/eng_periodical/datadivision.json',\n",
    "            '/home/jvdzwaan/data/icdar2017st/fr_monograph/datadivision.json',\n",
    "            '/home/jvdzwaan/data/icdar2017st/fr_periodical/datadivision.json')\n",
    "data_dirs = ('/home/jvdzwaan/data/icdar2017st/eng_monograph/aligned/',\n",
    "             '/home/jvdzwaan/data/icdar2017st/eng_periodical/aligned/',\n",
    "             '/home/jvdzwaan/data/icdar2017st/fr_monograph/aligned/',\n",
    "             '/home/jvdzwaan/data/icdar2017st/fr_periodical/aligned/')\n",
    "\n",
    "\n",
    "data = {'train': [], 'test': [], 'val': []}\n",
    "\n",
    "for ds, data_dir in zip(datasets, data_dirs):\n",
    "    #print(data_dir)\n",
    "    with open(ds) as d:\n",
    "        division = json.load(d)\n",
    "    \n",
    "    for n in division.keys():\n",
    "        for f in division[n]:\n",
    "            data[n].append(os.path.join(data_dir, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "create_context_data(data['train'], 'train-context')\n",
    "create_context_data(data['test'], 'test-context')\n",
    "create_context_data(data['val'], 'val-context')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import os\n",
    "\n",
    "datasets = '/home/jvdzwaan/data/icdar2017st/eng_monograph/datadivision.json'\n",
    "data_dir = '/home/jvdzwaan/data/icdar2017st/eng_monograph/aligned/'\n",
    "\n",
    "with open(datasets) as d:\n",
    "    division = json.load(d)\n",
    "\n",
    "create_context_data([os.path.join(data_dir, f) for f in division['train']], 'train-eng_mon-context')\n",
    "create_context_data([os.path.join(data_dir, f) for f in division['test']], 'test-eng_mon-context')\n",
    "create_context_data([os.path.join(data_dir, f) for f in division['val']], 'val-eng_mon-context')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [1, 2, 3]\n",
    "l[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(v.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for string in ocr_selected_val:\n",
    "    if len(string) > 53:\n",
    "        print(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ochre.datagen import DataGenerator\n",
    "\n",
    "dg_val = DataGenerator(xData=ocr_selected_val, yData=gs_selected_val, char_to_int=ci,\n",
    "                       seq_length=seq_length, padding_char='\\n', oov_char='@',\n",
    "                       batch_size=100, shuffle=False)\n",
    "dg_test = DataGenerator(xData=ocr_selected_test, yData=gs_selected_test, char_to_int=ci,\n",
    "                       seq_length=seq_length, padding_char='\\n', oov_char='@',\n",
    "                       batch_size=100, shuffle=False)\n",
    "dg_train = DataGenerator(xData=ocr_selected_train, yData=gs_selected_train, char_to_int=ci,\n",
    "                       seq_length=seq_length, padding_char='\\n', oov_char='@',\n",
    "                       batch_size=100, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import Embedding\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "n_nodes = 1000\n",
    "dropout = 0.2\n",
    "n_embed = 256\n",
    "n_vocab = len(ci)\n",
    "\n",
    "loss='categorical_crossentropy'\n",
    "optimizer='adam'\n",
    "metrics=['accuracy']\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# encoder\n",
    "\n",
    "model.add(Embedding(n_vocab, n_embed, input_length=seq_length))\n",
    "model.add(LSTM(n_nodes, input_shape=(seq_length, n_vocab)))\n",
    "# For the decoder's input, we repeat the encoded input for each time step\n",
    "model.add(RepeatVector(seq_length))\n",
    "model.add(LSTM(n_nodes, return_sequences=True))\n",
    "\n",
    "# For each of step of the output sequence, decide which character should be\n",
    "# chosen\n",
    "model.add(TimeDistributed(Dense(n_vocab, activation='softmax')))\n",
    "model.compile(loss=loss, optimizer=optimizer, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize saving of weights\n",
    "#filepath = os.path.join(weights_dir, '{loss:.4f}-{epoch:02d}.hdf5')\n",
    "filepath = '{loss:.4f}-{epoch:02d}.hdf5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1,\n",
    "                                 save_best_only=True, mode='min')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "# do training (and save weights)\n",
    "model.fit_generator(dg_train, steps_per_epoch=len(dg_train), epochs=10, \n",
    "                    validation_data=dg_val, \n",
    "                    validation_steps=len(dg_val), callbacks=callbacks_list,\n",
    "                    use_multiprocessing=True,\n",
    "                    workers=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
